{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2a239d",
   "metadata": {},
   "source": [
    " # <strong>Road networks and robustness to flooding on US Atlantic and Gulf barrier islands</strong>\n",
    " ## <strong>- Statistics -</strong>\n",
    " ### This notebook generates the stats included in the manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12b0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603af667",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=''\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e2d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Area and shoreline length of the US Atlantic and Gulf barrier islands\n",
    "\n",
    "# Calculate area (in km2) and shoreline length (in km) for all barriers\n",
    "barriers= gpd.read_file('./Data/Exceedance/US_barriers.shp')\n",
    "barriers= barriers.to_crs('esri:102003')\n",
    "barriers[\"area\"] = barriers['geometry'].area/ 10**6 \n",
    "barriers[\"length\"]= barriers['geometry'].length/10**3 \n",
    "\n",
    "# Filter those that belong to the 72 networks with more than 100 nodes (sampled) and keep the remaining in another df (unsampled)\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "sampled_barriers= list(table.Barrier) # >100 nodes\n",
    "sampled = barriers.query('name in @sampled_barriers')\n",
    "unsampled = barriers.query('name not in @sampled_barriers') #\n",
    "\n",
    "number_sampled= len(sampled_barriers) # number of barrier with more than 100 nodes\n",
    "\n",
    "# Stats\n",
    "area_sampled_sum=sampled['area'].sum()\n",
    "area_total_sum=barriers['area'].sum()\n",
    "area_sampled_mean=sampled['area'].mean()\n",
    "area_unsampled_mean=unsampled['area'].mean()\n",
    "area_sampled_100=len(sampled.loc[sampled['area']<100])\n",
    "area_sampled_100_perc=len(sampled.loc[sampled['area']<100])/number_sampled*100\n",
    "area_sampled_25=len(sampled.loc[sampled['area']<25])\n",
    "area_sampled_25_perc=len(sampled.loc[sampled['area']<25])/number_sampled*100\n",
    "\n",
    "length_sampled_sum=sampled['length'].sum()\n",
    "length_total_sum=barriers['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c87faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Street length\n",
    "\n",
    "rootdir = './Results/Statistics'\n",
    "extensions = ('.csv')\n",
    "\n",
    "barrier_names=[]\n",
    "length_street=[]\n",
    "\n",
    "# Loop through files with statistics and open each csv to retrieve street length\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".csv\",\"\")\n",
    "            barrier = barrier.replace(\"_geo\",\"\")\n",
    "            barrier_names.append(barrier)\n",
    "            table = pd.read_csv(file_path, sep=\",\", header=0)\n",
    "            table.rename(columns={ table.columns[0]: \"stats\", table.columns[1]:\"values\"}, inplace = True)\n",
    "            length=table.loc[table['stats'] == 'street_length_total', 'values'].iloc[0]/10**3\n",
    "            length_street.append(length)\n",
    "\n",
    "# Create new dataframe with results and filter those that have more than 100 nodes\n",
    "df = list(zip(barrier_names, length_street))\n",
    "df = pd.DataFrame(df, columns=['Barrier','Street_length'])\n",
    "\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "sampled_barriers= list(table.Barrier)\n",
    "sampled = df.query('Barrier in @sampled_barriers')\n",
    "\n",
    "# Stats\n",
    "street_length_min=sampled['Street_length'].min()\n",
    "street_length_max=sampled['Street_length'].max()\n",
    "street_length_mean=sampled['Street_length'].mean()\n",
    "street_length_200=len(sampled.loc[sampled['Street_length']>200])\n",
    "street_length_200_perc=len(sampled.loc[sampled['Street_length']>200])/number_sampled*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99db7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of nodes \n",
    "\n",
    "rootdir = './Data/Roads'\n",
    "extensions = ('.graphml')\n",
    "\n",
    "barrier_names=[]\n",
    "nodes=[]\n",
    "# Loop through files and open barrier graphml to retrieve number of nodes in each drivable road network\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            barrier_names.append(barrier)\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N= len(G.nodes(data=True))\n",
    "            nodes.append(N)\n",
    "\n",
    "df = list(zip(barrier_names, nodes))\n",
    "df= pd.DataFrame(df, columns=['Barrier','Nodes'])\n",
    "\n",
    "number_drivable=len(df)\n",
    "\n",
    "# Filter those that have more than 100 nodes (sampled)               \n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "sampled_barriers= list(table.Barrier)\n",
    "sampled = df.query('Barrier in @sampled_barriers')\n",
    "\n",
    "# Stats\n",
    "nodes_min=sampled['Nodes'].min()\n",
    "nodes_max=sampled['Nodes'].max()\n",
    "nodes_mean=sampled['Nodes'].mean()\n",
    "nodes_1000=len(sampled.loc[sampled['Nodes']>1000])\n",
    "nodes_1000_perc=len(sampled.loc[sampled['Nodes']>1000])/number_sampled*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e620ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes elevation\n",
    "\n",
    "rootdir = './Data/Roads'\n",
    "extensions = ('.graphml')\n",
    "\n",
    "elevations=[]\n",
    "barrier_names=[]\n",
    "\n",
    "# Loop through files and open barrier graphml to retrieve the elevation of each node in each drivable network\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            barrier_names.append(barrier)\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N= len(G.nodes(data=True))\n",
    "            # select only those that have more than 100 nodes\n",
    "            if N>100:\n",
    "                df=pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "                Elev= pd.to_numeric(df.Elevations)\n",
    "                Elev=list(Elev.values)\n",
    "                elevations.append(Elev)\n",
    "            \n",
    "merged_list = []\n",
    "for l in elevations:\n",
    "    merged_list += l\n",
    "elevations=np.array(merged_list)\n",
    "\n",
    "# Stats\n",
    "elevations_mean= elevations.mean()\n",
    "elevations_1btw3=len(elevations[(elevations>1)&(elevations<3)])/len(elevations)*100\n",
    "elevations_1=len(elevations[(elevations<1)])/len(elevations)*100\n",
    "elevations_5=len(elevations[(elevations>5)])/len(elevations)*100\n",
    "elevations_10=len(elevations[(elevations>10)])/len(elevations)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b6add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Critical elevation\n",
    "\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "df = table[['Barrier','Critical_elevation']]\n",
    "\n",
    "\n",
    "# Stats\n",
    "z_5=len(df.loc[df['Critical_elevation']<5])\n",
    "z_5_perc=len(df.loc[df['Critical_elevation']<5])/72*100\n",
    "z_25=len(df.loc[df['Critical_elevation']<2.5])\n",
    "z_25_perc=len(df.loc[df['Critical_elevation']<2.5])/72*100\n",
    "z_15=len(df.loc[df['Critical_elevation']<1.5])\n",
    "z_15_perc=len(df.loc[df['Critical_elevation']<1.5])/72*100\n",
    "z_1=len(df.loc[df['Critical_elevation']<1])\n",
    "z_1_perc=len(df.loc[df['Critical_elevation']<1])/72*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d0bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Critical exceedance\n",
    "\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "df = table[['Barrier','Critical_exceedance']]\n",
    "\n",
    "# Stats\n",
    "e_100=len(df.loc[df['Critical_exceedance']>0.01])\n",
    "e_100_perc=len(df.loc[df['Critical_exceedance']>0.01])/72*100\n",
    "e_10=len(df.loc[df['Critical_exceedance']>0.1])\n",
    "e_10_perc=len(df.loc[df['Critical_exceedance']>0.1])/72*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cecbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Critical elevation and critical exceedance\n",
    "\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "df = table[['Barrier','Critical_exceedance','Critical_elevation']]\n",
    "\n",
    "# Stats\n",
    "e_10=df.loc[df['Critical_exceedance']<10]\n",
    "e_10_meanz=e_10['Critical_elevation'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b8ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Robustness\n",
    "\n",
    "table = pd.read_csv(\"./Results/Results_AllBarriers.csv\", sep=\",\", header=0) \n",
    "df = table[['Barrier','Robustness']]\n",
    "\n",
    "# Stats\n",
    "R_03btw05= len(df.loc[(df['Robustness']>0.3)&(df['Robustness']<0.4)])\n",
    "R_03btw05_perc= len(df.loc[(df['Robustness']>0.3)&(df['Robustness']<0.4)])/72*100\n",
    "R_04= len(df.loc[df['Robustness']>0.4])\n",
    "R_04_perc= len(df.loc[df['Robustness']>0.4])/72*100\n",
    "R_045= len(df.loc[df['Robustness']>0.45])\n",
    "R_045_perc= len(df.loc[df['Robustness']>0.45])/72*100\n",
    "R_03= len(df.loc[df['Robustness']<0.3])\n",
    "R_03_perc= len(df.loc[df['Robustness']<0.3])/72*100\n",
    "R_02= len(df.loc[df['Robustness']<0.2])\n",
    "R_02_perc= len(df.loc[df['Robustness']<0.2])/72*100\n",
    "R_max=df.loc[df['Robustness']==df['Robustness'].max()]\n",
    "R_min=df.loc[df['Robustness']==df['Robustness'].min()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
