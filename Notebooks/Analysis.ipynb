{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4578cbf2",
   "metadata": {},
   "source": [
    " # <strong>Road networks and robustness to flooding on US Atlantic and Gulf barrier islands</strong>\n",
    " ## <strong>- Road network robustness to flooding -</strong>\n",
    " ### The purpose of this notebook is to identify, for each barrier island, the elevation and exceedance probability of the critical node that causes the network's failure and the overall robustness of each road network to flood-induced failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad54fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contextily as ctx\n",
    "import statistics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set working directory\n",
    "\n",
    "path='E:/Networks'\n",
    "# os.chdir(path) # In this notebook, this command cannot be used because it triggers a JSONDecodeError when GA9 is downloaded\n",
    "# To avoid the error and be able to download all road networks, the path to the working directory needs to be set as an absolute path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16091775",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explore the size decay of the GCC to identify the critical node that leads to the fragmentation of the network (road networks with more than 100 nodes) and plot maps with road networks\n",
    "\n",
    "# Create folders if they don't exist\n",
    "outdir= '{0}/Results'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "outdir= '{0}/Results/GCC_Plots'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "outdir= '{0}/Results/Networks_Maps'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "\n",
    "# Loop through files to open each barrier graphml\n",
    "rootdir = '{0}/Data/Roads2'.format(path)\n",
    "extensions = ('.graphml')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            print(barrier)\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N = len(G.nodes(data=True))\n",
    "            GCCs=[] # list with the number of connected components and its size\n",
    "            if N>100:\n",
    "                # pull out elevation attribute\n",
    "                Z = nx.get_node_attributes(G,'Elevations')\n",
    "                # convert str values in float to be able to sort them \n",
    "                Z = dict(zip(Z.keys(), [float(value) for value in Z.values()]))\n",
    "                # sort elevation values in ascending order\n",
    "                Sorted_Z = sorted(Z.items(), key=lambda item: item[1])\n",
    "                # select first element of each tuple in the list (nodes ID):\n",
    "                FT = [i[0] for i in Sorted_Z]\n",
    "                # Select second element of each tuple in the list (elevation) and convert to float\n",
    "                ST = [i[1] for i in Sorted_Z]\n",
    "                for i in range(len(ST)):\n",
    "                    ST[i] = float(ST[i])\n",
    "                # create array \n",
    "                CCs = np.zeros([len(Sorted_Z),2])\n",
    "                # loop through all nodes\n",
    "                for i in range(0, len(FT)):\n",
    "                    # find the node with lowest elevation from the list using i and remove it\n",
    "                    G.remove_nodes_from(FT[0:i])\n",
    "                    # find the number of connected components and its respective size\n",
    "                    GCC = [len(c)\n",
    "                            for c in sorted(nx.weakly_connected_components(G), key=len, reverse=True)]\n",
    "                    GCCs.append(GCC) \n",
    "                    # fill array, first column corresponds to FGC (first giant component), second column to SGC (second giant component)\n",
    "                    if len(GCC)==1:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=0\n",
    "                    else:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=GCC[1]/len(FT)\n",
    "                # find the node that, once removed, the size of the FGC drops abruptly while the size of the SGC reaches its maximum\n",
    "                m = max(CCs[:,1])\n",
    "                pos=[i for i, j in enumerate(CCs[:,1]) if j == m]\n",
    "                pos= pos[0] # position of max value in SGC\n",
    "                critical= pos-1 # position of the critical node whose removal causes the percolation transition.\n",
    "                elev=ST[critical] # find elevation of the critical node\n",
    "                removed=pos # number of nodes removed when percolation threshold occurs\n",
    "\n",
    "                # plot\n",
    "                col1=[] \n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col1.append('#D53032') \n",
    "                    else:\n",
    "                        col1.append('#000000')  \n",
    "                col2=[]\n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col2.append('#D53032') \n",
    "                    else:\n",
    "                        col2.append('#808080') \n",
    "                col3=[]\n",
    "                for i in range(0,len(FT)):\n",
    "                    if i==critical:\n",
    "                        col3.append('#D53032') \n",
    "                    else:\n",
    "                        col3.append('#9ACD32') \n",
    "\n",
    "                f, (ax1,ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "                x_coord = 1 * np.arange(len(FT))/len(FT) # fraction of nodes removed\n",
    "                ax1.plot(x_coord, CCs[:,0],':ok') # FGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax1.plot(x_coord[i],CCs[i,0],'o', markersize=5, color=col1[i]) # plot with two colors to highlight critical node\n",
    "                ax1.set_ylabel(\"First Giant Component Size\")\n",
    "                ax3 = ax1.twinx()\n",
    "                ax3.plot(x_coord, CCs[:,1],':ok') # SGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax3.plot(x_coord[i],CCs[i,1],'o', markersize=5, color=col2[i]) \n",
    "                ax3.set_ylabel(\"Second Giant Component Size\")\n",
    "                ax2.plot(x_coord,CCs[:,0],':ok') # FGC\n",
    "                for i in range (len(FT)):\n",
    "                    ax2.plot(x_coord[i],CCs[i,0],'o', markersize=5, color=col1[i]) \n",
    "                ax2.set_ylabel(\"First Giant Component Size\")\n",
    "                ax4 = ax2.twinx()\n",
    "                ax4.plot(x_coord,ST,':o', color='#9ACD32') # elevation\n",
    "                for i in range (len(FT)):\n",
    "                    ax4.plot(x_coord[i],ST[i],'o', markersize=5, color=col3[i]) \n",
    "                ax2.set_ylabel(\"First Giant Component Size\",)\n",
    "                ax4.set_ylabel(\"Elevation\")\n",
    "                ax2.set_xlabel(\"Fraction of removed nodes\") \n",
    "                legend_elements1 = [Line2D([0], [0], marker='o', color='#000000', label='FGC', markersize=10),\n",
    "                                    Line2D([0], [0], marker='o', color='#808080', label='SGC', markersize=10),\n",
    "                                    Line2D([0], [0], marker='o', color='#D53032', label='Critical node', markersize=10)]\n",
    "                ax1.legend(handles=legend_elements1, loc=\"best\", frameon=False, fontsize=18)\n",
    "                legend_elements2 = [Line2D([0], [0], marker ='o', color='#000000', label='FGC', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='#9ACD32', label='Elevation', markersize=10)]\n",
    "                ax1.legend(handles=legend_elements1, loc=\"best\", frameon=False, fontsize=18)\n",
    "\n",
    "                plt.rcParams[\"font.size\"]= 20\n",
    "                plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "                f.savefig(\"{0}/Results/GCC_Plots/{1}.png\".format(path,barrier), dpi=500, facecolor='w')\n",
    "                plt.close(\"all\")\n",
    "\n",
    "                ### create maps for each network using OSM as basemap \n",
    "\n",
    "                # read polygons\n",
    "                poly = gpd.read_file(\"{0}/Data/Barriers/Barriers_AtlGulf/{1}_geo.shp\".format(path,barrier))\n",
    "                # extract just the geometry (shapely object) part and clean it with a buffer\n",
    "                poly_geo = poly['geometry'].iloc[0]\n",
    "                poly_geo = poly_geo.buffer(0)\n",
    "                poly_geo.is_valid\n",
    "                # extract drivable network and project it\n",
    "                graph = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "                # retrieve nodes and edges as geodataframes\n",
    "                nodes, edges = ox.graph_to_gdfs(graph)\n",
    "                # create an index for the geodataframe nodes\n",
    "                nodes['index'] = range(0, len(nodes))\n",
    "\n",
    "                # convert Z dict in pandas dataframe and name columns\n",
    "                Z = pd.DataFrame(list(Z.items()),columns = ['index','elevation'])\n",
    "                # convert all columns in numerics so there are no errors when merging\n",
    "                Z = Z.apply(pd.to_numeric)\n",
    "                # join pandas dataframe to nodes geodataframe using 'index' so that the gdf has elevation\n",
    "                nodes = nodes.merge(Z, on='index')\n",
    "\n",
    "                # create new columns for color and size\n",
    "                def color(row):\n",
    "                    if row['elevation'] < elev:\n",
    "                        val = \"black\"\n",
    "                    elif row['elevation']== elev:\n",
    "                        val = \"red\"\n",
    "                    else:\n",
    "                        val = \"green\"\n",
    "                    return val\n",
    "\n",
    "                def size(row):\n",
    "                    if row['elevation'] == elev:\n",
    "                        val = 50\n",
    "                    else:\n",
    "                        val = 30\n",
    "                    return val\n",
    "\n",
    "                nodes['Color'] = nodes.apply(color, axis=1) # new column with color categories \n",
    "                nodes['Size'] = nodes.apply(size, axis=1) # new column with size categories\n",
    "\n",
    "                # plot map\n",
    "                fig, ax = plt.subplots()\n",
    "                nodes = nodes.to_crs(epsg=3857) # convert gdf to EPSG used by basemaps\n",
    "                edges = edges.to_crs(epsg=3857)\n",
    "                nodes.plot(ax=ax, color=nodes.Color, markersize=nodes.Size, zorder=2, legend=True) # plot nodes\n",
    "                edges.plot(ax=ax, alpha=0.2, color='black', zorder=1) # plot edges\n",
    "                ctx.add_basemap(ax, zoom=13, source=ctx.providers.OpenStreetMap.Mapnik) # add basemap (OSM)\n",
    "                plt.xticks(fontsize=12) # reduce fontsize of x axis\n",
    "                plt.yticks(fontsize=12) # reduce fontsize of y axis\n",
    "                legend_elements = [Line2D([0], [0], marker='o', color='black', label='Connected nodes',\n",
    "                                          markerfacecolor='g', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='black', label='Disconnected nodes',\n",
    "                                          markerfacecolor='b', markersize=10),\n",
    "                                   Line2D([0], [0], marker='o', color='black', label='Target node',\n",
    "                                          markerfacecolor='r', markersize=10),\n",
    "                                   ] # create legend\n",
    "                ax.legend(handles=legend_elements, loc='best', frameon=False)\n",
    "                ax.set_title(barrier, fontsize=22)\n",
    "                ax.ticklabel_format(style='plain')         \n",
    "                plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "                plt.savefig('{0}/Results/Networks_Maps/{1}.png'.format(path,barrier), dpi=300, facecolor='w')   \n",
    "                plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63fb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create table with results for all barriers with drivable networks \n",
    "\n",
    "barriers=[] # barrier name\n",
    "n_nodes=[] # number of nodes\n",
    "r=[] # robustness \n",
    "min_z=[] # min node elevation in the network\n",
    "max_z=[] # max node elevation in the network\n",
    "mean_z=[] # mean node elevation\n",
    "median_z=[] # median node elevation\n",
    "critical_z=[] # elevation critical node\n",
    "critical_e=[] # exceedance probability critical node (given in return period)\n",
    "removed_nodes=[] # number of nodes removed when critical node is removed\n",
    "removed_perc=[] # percentage of nodes removed when critical node is removed\n",
    "threshold=[] # value critical threshold\n",
    "\n",
    "\n",
    "rootdir = '{0}/Data/Roads'.format(path)\n",
    "extensions = ('.graphml')\n",
    "\n",
    "# Loop through files and open barrier graphml\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".graphml\",\"\")\n",
    "            G = nx.read_graphml(file_path)\n",
    "            N= len(G.nodes(data=True))\n",
    "            if N>100:\n",
    "                GCCs=[]\n",
    "                barriers.append(barrier) \n",
    "                n_nodes.append(N) \n",
    "                # pull out elevation attribute\n",
    "                Z = nx.get_node_attributes(G,'Elevations')\n",
    "                # convert str values in float to be able to sort them \n",
    "                Z = dict(zip(Z.keys(), [float(value) for value in Z.values()]))\n",
    "                # sort it based on elevation, min first\n",
    "                Sorted_Z = sorted(Z.items(), key=lambda item: item[1])\n",
    "                CCs = np.zeros([len(Sorted_Z),2])\n",
    "                # select first element of each tuple in the list (nodes ID):\n",
    "                FT = [i[0] for i in Sorted_Z]\n",
    "                # select second element of each tuple in the list (elevation) and convert to float\n",
    "                ST = [i[1] for i in Sorted_Z]\n",
    "                for i in range(len(ST)):\n",
    "                    ST[i] = float(ST[i])\n",
    "\n",
    "                # calculate elevation stats \n",
    "                min_elev=min(ST)\n",
    "                min_z.append(min_elev)\n",
    "                max_elev=max(ST)\n",
    "                max_z.append(max_elev)\n",
    "                mean_elev = statistics.mean(ST)\n",
    "                mean_z.append(mean_elev)\n",
    "                median_elev = statistics.median(ST)\n",
    "                median_z.append(median_elev)\n",
    "                \n",
    "                # remove nodes by elevation and calculate size of first and second components\n",
    "                for i in range(0, len(FT)):\n",
    "                    # find the node with lowest elevation from the list using i and remove it\n",
    "                    G.remove_nodes_from(FT[0:i])\n",
    "                    # find the number of connected components and its respective size\n",
    "                    GCC = [len(c)\n",
    "                            for c in sorted(nx.weakly_connected_components(G), key=len, reverse=True)]\n",
    "                    GCCs.append(GCC) # list with the number of connected components and its size\n",
    "                    # fill array, first column corresponds to FGC (first giant component), second column to SGC (second giant component)\n",
    "                    if len(GCC)==1:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=0\n",
    "                    else:\n",
    "                        CCs[int(i),0]=GCC[0]/len(FT)\n",
    "                        CCs[int(i),1]=GCC[1]/len(FT)\n",
    "\n",
    "                # find the node that, once removed, the FGC decreases and the SGC reaches its maximum (critical threshold)\n",
    "                m = max(CCs[:,1])\n",
    "                pos=[i for i, j in enumerate(CCs[:,1]) if j == m]\n",
    "                pos= pos[0] # position of max value in SGC\n",
    "                critical= pos-1 # position of the critical node whose removal causes the percolation transition.\n",
    "                elev=ST[critical] # find elevation of the critical node\n",
    "                critical_z.append(elev)\n",
    "                removed=pos # number of nodes removed when percolation threshold occurs\n",
    "                removed_nodes.append(removed)\n",
    "                perc_removed=int(removed)/N*100\n",
    "                removed_perc.append(perc_removed)\n",
    "                x_coord = 1 * np.arange(len(FT))/len(FT) # Fraction of nodes removed\n",
    "                thresh= x_coord[critical]\n",
    "                threshold.append(thresh)\n",
    "\n",
    "                # exceedance probability for the critical node\n",
    "                exceed = pd.read_csv(\"{0}/Data/Exceedance/Probability/{1}_Exceedance.csv\".format(path,barrier), sep=\",\", header=0)\n",
    "                exceed_x= exceed.MaxWL\n",
    "                exceed_y= exceed.Return_Pd\n",
    "                node_elev= elev\n",
    "                exceedance= np.interp(node_elev, exceed_x, exceed_y)\n",
    "                critical_e.append(exceedance)\n",
    "\n",
    "                # calculate robustness following Schneider's equation (2011) \n",
    "                s= sum(CCs[:,0])\n",
    "                rob= s/len(FT)\n",
    "                r.append(rob)\n",
    "            else:\n",
    "                continue\n",
    "table = list(zip(barriers,n_nodes,r,min_z,max_z,mean_z,median_z,critical_z,critical_e,removed_nodes,removed_perc,threshold))\n",
    "table = pd.DataFrame(table, columns=['Barrier','Nodes','Robustness','Min_elevation','Max_elevation','Mean_elevation','Median_elevation','Critical_elevation','Critical_exceedance','Removed_nodes','Removed_%','Critical_threshold'])\n",
    "table.to_csv('{0}/Results/Results_AllBarriers.csv'.format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For each network, calculate basic statistics using OSMnx package\n",
    "\n",
    "# Create folders if it doesn't exist\n",
    "outdir= '{0}/Results/Statistics'.format(path)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "rootdir = \"{0}/Data/Barriers\\Barriers_AtlGulf\".format(path)\n",
    "extensions = ('.shp')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            barrier = file.replace(\".shp\",\"\")\n",
    "            print(barrier)\n",
    "            # read polygons\n",
    "            poly = gpd.read_file(file_path)\n",
    "            # extract just the geometry (shapely object) part and clean it with a buffer\n",
    "            poly_geo = poly['geometry'].iloc[0]\n",
    "            poly_geo = poly_geo.buffer(0)\n",
    "            poly_geo.is_valid\n",
    "            # project polygon to calculate area\n",
    "            poly_prj=ox.project_gdf(poly)\n",
    "            area=float(poly_prj.area)\n",
    "            \n",
    "            try:\n",
    "                # pull network\n",
    "                G = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "                if len(G.nodes(data=True))>100:\n",
    "                    # project it and calculate statistics\n",
    "                    G_proj = ox.project_graph(G)\n",
    "                    stats = ox.basic_stats(G_proj, area=area, circuity_dist='euclidean')\n",
    "\n",
    "                    # delete the no longer needed dict elements\n",
    "                    del stats['streets_per_node_counts']\n",
    "                    del stats['streets_per_node_proportion']\n",
    "\n",
    "                    # load as a pandas dataframe\n",
    "                    df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "                    df.columns= [barrier]\n",
    "                    df.to_csv('{0}/Results/Statistics/{1}.csv'.format(path,barrier))\n",
    "        \n",
    "            except:\n",
    "                continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
