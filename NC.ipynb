{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import packages needed to extract road network using a polygon\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Import packages to extract buildings\n",
    "import matplotlib.pyplot as plt\n",
    "import osmnx as ox\n",
    "from IPython.display import Image\n",
    "\n",
    "# Other packages\n",
    "import networkx as nx # to work with graphs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import urllib3\n",
    "import scipy as sp # for the adjacency matrix\n",
    "\n",
    "%matplotlib inline\n",
    "ox.config(log_console=True, use_cache=True)\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Extract networks using barrier polygons, extract USGS elevation for each node and save as graphml #####\n",
    "\n",
    "# Function to keept trying when making elevations request\n",
    "def make_remote_request(url: str, params: dict, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Makes the remote request\n",
    "    Continues making attempts until it succeeds\n",
    "    \"\"\"\n",
    "\n",
    "    count = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get((url + urllib.parse.urlencode(params)))\n",
    "        except (OSError, urllib3.exceptions.ProtocolError) as error:\n",
    "            print('\\n')\n",
    "            print('*' * 20, 'Error Occured', '*' * 20)\n",
    "            print(f'Number of tries: {count}')\n",
    "            print(f'URL: {url}')\n",
    "            print(error)\n",
    "            print('\\n')\n",
    "            count += 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    return response\n",
    "\n",
    "# Function to extract elevation using coordinates\n",
    "def elevation_function(x):\n",
    "    url = 'https://nationalmap.gov/epqs/pqs.php?'\n",
    "    params = {'x': x[1],\n",
    "              'y': x[0],\n",
    "              'units': 'Meters',\n",
    "              'output': 'json'}\n",
    "    result = make_remote_request(url, params)\n",
    "    return result.json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation']\n",
    "\n",
    "url = r'https://nationalmap.gov/epqs/pqs.php?' # USGS Elevation Point Query Service\n",
    "\n",
    "# Loop within polygons folder and extract network for each barrier\n",
    "rootdir = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Polygons'\n",
    "extensions = ('.shp')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            print (os.path.join(subdir, file))\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Read polygons\n",
    "                poly = geopandas.read_file(file_path)\n",
    "                # Extract just the geometry (shapely object) part\n",
    "                poly_geo = poly['geometry'].iloc[0]\n",
    "                # Clean it with a buffer\n",
    "                poly_geo = poly_geo.buffer(0)\n",
    "                \n",
    "                # Pull the roads with OSMnx\n",
    "                G = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "                print(len(G.nodes))\n",
    "                # Give to each node a new index based on integers from 0 and then add the osmid as an attribute\n",
    "                osmids = list(G.nodes)\n",
    "                G = nx.relabel.convert_node_labels_to_integers(G)\n",
    "                osmid_values = {k:v for k, v in zip(G.nodes, osmids)}\n",
    "                nx.set_node_attributes(G, osmid_values, 'osmid')\n",
    "                \n",
    "                # Create dataframe with lat and lon values to use with the elevation function\n",
    "                Y = nx.get_node_attributes(G,'y')\n",
    "                X= nx.get_node_attributes(G,'x')\n",
    "                Ydf = pd.DataFrame.from_dict(Y, orient='index', columns=['lat'])\n",
    "                Xdf = pd.DataFrame.from_dict(X, orient='index', columns=['lon'])\n",
    "                df = pd.concat([Ydf, Xdf], axis=1)\n",
    "\n",
    "                # Apply the function\n",
    "                df['elevations'] = df.apply(elevation_function, axis=1)\n",
    "                # Add elevation as a new attribute\n",
    "                nx.set_node_attributes(G, df.elevations, 'Elevations')\n",
    "\n",
    "                # Save street network as graph\n",
    "                ox.save_graphml(G, filepath='C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Graph_Elevation\\\\{0}.graphml'.format(file))\n",
    "                #nodes.to_file(driver = 'ESRI Shapefile', filename = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\Elevation\\\\{0}'.format(file))\n",
    "                print(\"done\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GCS behavior - individual barriers\n",
    "\n",
    "            \n",
    "G = nx.read_graphml(r\"C:\\Users\\Sofia\\Dropbox\\Barriers_US\\NorthCarolina\\Graph_Elevation\\NC22.graphml\")\n",
    "# Make a copy of the network to break\n",
    "B = G\n",
    "\n",
    "# Pull out elevation attribute\n",
    "E = nx.get_node_attributes(G,'Elevations')\n",
    "# Convert str values in float to be able to sort them \n",
    "E = dict(zip(E.keys(), [float(value) for value in E.values()]))\n",
    "# Sort it based on elevation, min first\n",
    "Sorted_E = sorted(E.items(), key=lambda item: item[1])\n",
    "CCs = np.zeros([len(Sorted_E),2])\n",
    "# Select first element of each tuple in the list (nodes ID):\n",
    "FT = [i[0] for i in Sorted_E]\n",
    "ST = [i[1] for i in Sorted_E]\n",
    "for i in range(len(ST)):\n",
    "    ST[i] = float(ST[i])\n",
    "\n",
    "# Loop through all nodes\n",
    "for i in range(0, len(FT)):\n",
    "    # Find the node with lowest elevation from the list using i and remove it\n",
    "    B.remove_nodes_from(FT[0:i])\n",
    "    # Check number of connected components/clusters\n",
    "    CCs[int(i),0] = nx.algorithms.components.number_weakly_connected_components(B)\n",
    "    # Check giant component size\n",
    "    CCs[int(i),1]= largest_cc = len(max(nx.weakly_connected_components(B), key=len))/len(FT)\n",
    "\n",
    "# Plot\n",
    "x_coord = 1 * np.arange(len(CCs))/len(CCs)\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(x_coord,CCs[:,0],':ok')\n",
    "ax1.set_ylabel(\"Number connected components\")\n",
    "ax2.plot(x_coord,CCs[:,1],':s', color='#4682B4', label='Giant component size')\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot(x_coord,ST,':o', color='#9ACD32', label='Elevation')\n",
    "\n",
    "\n",
    "ax2.set_ylabel(\"Giant Component Size\",)\n",
    "ax3.set_ylabel(\"Elevation\")\n",
    "ax2.set_xlabel(\"Fraction of removed nodes\") \n",
    "ax1.set_title('NC22', fontsize=22)\n",
    "\n",
    "\n",
    "f.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 0.45), ncol=2, frameon=False)\n",
    "\n",
    "f.tight_layout()\n",
    "\n",
    "# Calculate robustness following Schneider's equation (2011)\n",
    "\n",
    "s= sum(CCs[:,1])\n",
    "r= s/len(FT)\n",
    "print(r)\n",
    "\n",
    "# Add text with R value\n",
    "plt.text( 0.75, 0.20, r'R=0.42',\n",
    "    ha='left', va='top',\n",
    "    transform=f.transFigure\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.show()\n",
    "plt.rcParams[\"font.size\"]= 20\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "\n",
    "# Save figure\n",
    "f.savefig(\"C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Figures\\\\NC22.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator to plot connected components, GCS and elevation for all barriers in North Carolina\n",
    "\n",
    "rootdir = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\NETWORKS\\\\NorthCarolina\\\\Graph_Elevation'\n",
    "extensions = ('.graphml')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            print (os.path.join(subdir, file))\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            G = nx.read_graphml(file_path)\n",
    "            # Make a copy of the network to break\n",
    "            B = G\n",
    "\n",
    "            # Pull out elevation attribute\n",
    "            E = nx.get_node_attributes(G,'Elevations')\n",
    "            # Convert str values in float to be able to sort them \n",
    "            E = dict(zip(E.keys(), [float(value) for value in E.values()]))\n",
    "            # Sort it based on elevation, min first\n",
    "            Sorted_E = sorted(E.items(), key=lambda item: item[1])\n",
    "            CCs = np.zeros([len(Sorted_E),2])\n",
    "            # Select first element of each tuple in the list (nodes ID):\n",
    "            FT = [i[0] for i in Sorted_E]\n",
    "            ST = [i[1] for i in Sorted_E]\n",
    "            for i in range(len(ST)):\n",
    "                ST[i] = float(ST[i])\n",
    "\n",
    "            # Loop through all nodes\n",
    "            for i in range(0, len(FT)):\n",
    "                # Find the node with lowest elevation from the list using i and remove it\n",
    "                B.remove_nodes_from(FT[0:i])\n",
    "                # Check number of connected components/clusters\n",
    "                CCs[int(i),0] = nx.algorithms.components.number_weakly_connected_components(B)\n",
    "                # Check giant component size\n",
    "                CCs[int(i),1]= largest_cc = len(max(nx.weakly_connected_components(B), key=len))/len(FT)\n",
    "\n",
    "            # Plot\n",
    "            x_coord = 1 * np.arange(len(CCs))/len(CCs)\n",
    "            f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "            ax1.plot(x_coord,CCs[:,0],':ok')\n",
    "            ax1.set_ylabel(\"Number connected components\")\n",
    "            ax2.plot(x_coord,CCs[:,1],':s', color='#4682B4', label='Giant component size')\n",
    "            ax3 = ax2.twinx()\n",
    "            ax3.plot(x_coord,ST,':o', color='#9ACD32', label='Elevation')\n",
    "\n",
    "\n",
    "            ax2.set_ylabel(\"Giant Component Size\",)\n",
    "            ax3.set_ylabel(\"Elevation\")\n",
    "            ax2.set_xlabel(\"Fraction of removed nodes\") \n",
    "            title= file.replace('.graphml', '')\n",
    "            ax1.set_title(title, fontsize=22)\n",
    "\n",
    "\n",
    "            f.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 0.45), ncol=2, frameon=False)\n",
    "        \n",
    "            f.tight_layout()\n",
    "\n",
    "            # Calculate robustness following Schneider's equation (2011)\n",
    "\n",
    "            s= sum(CCs[:,1])\n",
    "            r= round(s/len(FT),2)\n",
    "            \n",
    "\n",
    "            # Add text with R value\n",
    "            plt.text( 0.75, 0.20, r'R=%s'%(r),\n",
    "                ha='left', va='top',\n",
    "                transform=f.transFigure\n",
    "            )\n",
    "\n",
    "            # Plot\n",
    "            plt.show()\n",
    "            plt.rcParams[\"font.size\"]= 20\n",
    "            plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "            # Save figure\n",
    "            f.savefig(\"C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Figures\\\\{0}.jpeg\".format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate area of barrier polygons ####\n",
    "\n",
    "# Loop within polygons folder and extract network for each barrier\n",
    "rootdir = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Polygons'\n",
    "extensions = ('.shp')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            print (os.path.join(subdir, file))\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            \n",
    "            # Read polygons\n",
    "            poly = gpd.read_file(file_path)\n",
    "            poly_prj = poly.copy()\n",
    "            poly_prj = poly_prj.to_crs(\"EPSG:6542\")\n",
    "            poly_prj[\"Area_km2\"] = poly_prj['geometry'].area/10**6\n",
    "            print(poly_prj[\"Area_km2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate statistics ####\n",
    "\n",
    "# Read polygons\n",
    "poly = gpd.read_file(\"C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\Polygons\\\\NC22_geo.shp\")\n",
    "# Extract just the geometry (shapely object) part\n",
    "poly_geo = poly['geometry'].iloc[0]\n",
    "# Clean it with a buffer\n",
    "poly_geo = poly_geo.buffer(0)\n",
    "poly_geo.is_valid\n",
    "# Project polygon and calculate area\n",
    "poly_prj=ox.project_gdf(poly)\n",
    "area=poly_prj.area\n",
    "print(area/10**6)\n",
    "\n",
    "# Pull network\n",
    "G = ox.graph_from_polygon(poly_geo, network_type='drive', simplify=True, clean_periphery=True)\n",
    "\n",
    "# Calculate network area based on convex hull\n",
    "G_proj = ox.project_graph(G)\n",
    "nodes_proj = ox.graph_to_gdfs(G_proj, edges=False)\n",
    "graph_area_m = nodes_proj.unary_union.convex_hull.area\n",
    "print(graph_area_m/10**6)\n",
    "\n",
    "# show some basic stats about the network\n",
    "stats = ox.basic_stats(G_proj, area=graph_area_m, clean_intersects=True, circuity_dist='euclidean')\n",
    "\n",
    "# delete the no longer needed dict elements\n",
    "del stats['streets_per_node_counts']\n",
    "del stats['streets_per_node_proportion']\n",
    "\n",
    "# load as a pandas dataframe\n",
    "df = pd.DataFrame(pd.Series(stats, name='value'))\n",
    "df.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended stats - I haven't run this yet, that's just the code to do it.\n",
    "extended_stats = ox.extended_stats(G, ecc=True, bc=True, cc=True)\n",
    "for key, value in extended_stats.items():\n",
    "    stats[key] = value\n",
    "pd.Series(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Extract building footprints and save them as shp ####\n",
    "\n",
    "rootdir = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Polygons'\n",
    "extensions = ('.shp')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        try:\n",
    "            ext = os.path.splitext(file)[-1].lower()\n",
    "            if ext in extensions:\n",
    "                print (os.path.join(subdir, file))\n",
    "                file_path = os.path.join(subdir, file)\n",
    "            \n",
    "                # import polygon\n",
    "                polygon = gpd.read_file(file_path)     \n",
    "                # extract just the geometry (shapely object) part\n",
    "                geom = polygon['geometry'].iloc[0]         \n",
    "                # make geometry valid\n",
    "                geom = geom.buffer(0)\n",
    "                print(geom.is_valid)\n",
    "            \n",
    "                # extract buildings using polygon and save footprints as shp\n",
    "                footprints = ox.geometries_from_polygon(geom, tags={'building':True})\n",
    "                # project footprints\n",
    "                footprints = ox.project_gdf(footprints)\n",
    "                # get indexes where name column has value 'node'\n",
    "                points = footprints[footprints['element_type'] == 'node'].index\n",
    "                # delete these row indexes from dataFrame\n",
    "                footprints.drop(points , inplace=True)\n",
    "                \n",
    "                # calculate areas in square meters\n",
    "                footprints[\"Area_m2\"] = footprints.area\n",
    "               \n",
    "                # save footprints as shp\n",
    "                footprints_save = footprints.applymap(lambda x: str(x) if isinstance(x, list) else x)\n",
    "                footprints_save.drop(labels='nodes', axis=1).to_file('C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Buildings2\\\\{0}'.format(file))\n",
    "            \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate number of buildings and total building area ####\n",
    "\n",
    "rootdir = 'C:\\\\Users\\\\Sofia\\\\Dropbox\\\\Barriers_US\\\\NorthCarolina\\\\Buildings'\n",
    "extensions = ('.shp')\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[-1].lower()\n",
    "        if ext in extensions:\n",
    "            print (os.path.join(subdir, file))\n",
    "            file_path = os.path.join(subdir, file)\n",
    "\n",
    "            footprints= gpd.read_file(file_path)\n",
    "            print(len(footprints))\n",
    "            total_area = sum(footprints[\"Area_m2\"])\n",
    "            print(total_area/10**6)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Connectivity: Remove nodes until the network breaks ####\n",
    "#This has the problem that once one single node is isolated, the whole network disconnects, even though there is still a giant component and the network is still viable\n",
    "\n",
    "G = nx.read_graphml(r\"C:\\Users\\Sofia\\Dropbox\\Barriers_US\\NorthCarolina\\Graph_Elevation\\NC4_geo.shp.graphml\")\n",
    "\n",
    "#make a copy of the network to break\n",
    "B = G\n",
    "\n",
    "\n",
    "# pull out elevation attribute \n",
    "E = nx.get_node_attributes(B,'Elevations')\n",
    "#sort it based on elevation, min first\n",
    "Sorted_E = sorted(E.items(), key=lambda item: item[1])\n",
    "#Start at first entry of the list\n",
    "i = 0\n",
    "#assume you start with a connected network\n",
    "#loop through until the network breaks\n",
    "while True:\n",
    "    #find the node with lowest elevation from the list using i and remove it\n",
    "    B.remove_node((Sorted_E[i][0]))\n",
    "    print(i)\n",
    "    #check for connection\n",
    "    NotConnected = nx.algorithms.components.is_weakly_connected(B)\n",
    "    if NotConnected == False:\n",
    "        #Stop loop\n",
    "        break\n",
    "  #increment the index\n",
    "    i+= 1\n",
    "# print the node that broke the system\n",
    "print(i)\n",
    "print(Sorted_E[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram to with elevations\n",
    "\n",
    "G = nx.read_graphml(r\"C:\\Users\\Sofia\\Dropbox\\Barriers_US\\NorthCarolina\\Graph_Elevation\\NC4.graphml\")\n",
    "\n",
    "# Convert network nodes into dataframe for plotting\n",
    "G2 = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "Elev= pd.to_numeric(G2.Elevations)\n",
    "# Stair histogram for elevations\n",
    "\n",
    "def find_bins(observations, width):\n",
    "    minimmum = np.min(observations)\n",
    "    maximmum = np.max(observations)\n",
    "    bound_min = -1.0 * (minimmum % width - minimmum)\n",
    "    bound_max = maximmum - maximmum % width + width\n",
    "    n = int((bound_max - bound_min) / width) + 1\n",
    "    bins = np.linspace(bound_min, bound_max, n)\n",
    "    return bins\n",
    "\n",
    "bins = find_bins(Elev, 0.5)\n",
    "\n",
    "fig = plt.hist(Elev, histtype='step', bins=bins)\n",
    "\n",
    "plt.title('NC4')\n",
    "plt.xlabel(\"Elevation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
